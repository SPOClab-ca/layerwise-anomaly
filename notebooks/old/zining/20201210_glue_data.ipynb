{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GLUE data\n",
    "Let's see if the LMs are aware of some meta-task incoherences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "import numpy as np\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMaskedLM were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'sequence': \"[CLS] This is the best thing I've done in my life. [SEP]\",\n",
       "  'score': 0.4526607096195221,\n",
       "  'token': 1694,\n",
       "  'token_str': 'done'},\n",
       " {'sequence': \"[CLS] This is the best thing I've said in my life. [SEP]\",\n",
       "  'score': 0.12792159616947174,\n",
       "  'token': 1163,\n",
       "  'token_str': 'said'},\n",
       " {'sequence': \"[CLS] This is the best thing I've heard in my life. [SEP]\",\n",
       "  'score': 0.08337395638227463,\n",
       "  'token': 1767,\n",
       "  'token_str': 'heard'},\n",
       " {'sequence': \"[CLS] This is the best thing I've had in my life. [SEP]\",\n",
       "  'score': 0.07100346684455872,\n",
       "  'token': 1125,\n",
       "  'token_str': 'had'},\n",
       " {'sequence': \"[CLS] This is the best thing I've seen in my life. [SEP]\",\n",
       "  'score': 0.044269781559705734,\n",
       "  'token': 1562,\n",
       "  'token_str': 'seen'}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "nlp = pipeline(\"fill-mask\", model=\"bert-base-cased\")\n",
    "nlp(f\"This is the best thing I've {nlp.tokenizer.mask_token} in my life.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sequence': '[CLS] This movie is great and my kids loved it. This is a positive review [SEP]',\n",
       "  'score': 0.0781143307685852,\n",
       "  'token': 3112,\n",
       "  'token_str': 'positive'},\n",
       " {'sequence': '[CLS] This movie is great and my kids loved it. This is a negative review [SEP]',\n",
       "  'score': 0.012503745965659618,\n",
       "  'token': 4366,\n",
       "  'token_str': 'negative'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp(f\"This movie is great and my kids loved it. This is a {nlp.tokenizer.mask_token} review\", targets=[' positive', ' negative'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sequence': '[CLS] My kids thought this movie is disgusting. This is a negative review [SEP]',\n",
       "  'score': 0.06674962490797043,\n",
       "  'token': 4366,\n",
       "  'token_str': 'negative'},\n",
       " {'sequence': '[CLS] My kids thought this movie is disgusting. This is a positive review [SEP]',\n",
       "  'score': 0.04843594878911972,\n",
       "  'token': 3112,\n",
       "  'token_str': 'positive'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp(f\"My kids thought this movie is disgusting. This is a {nlp.tokenizer.mask_token} review\", targets=[' positive', ' negative'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sequence': \"[CLS] I admit I had some trepidation when I first saw the previews for this film. Was VH - 1 treading on hollow ground here? I mean, Harris and Quinn don't really look or even sound like John or Paul. But I have to admit, this film really surprised me. It's far from the exploitation film I expected. Instead, it's a character study, a low - key, whimsical, and ultimately bittersweet look at friendship, and the ultimate lesson we all learn : it's hard, if not impossible, to capture what we once had, and what has passed us by. This is a positive review [SEP]\",\n",
       "  'score': 0.12882345914840698,\n",
       "  'token': 3112,\n",
       "  'token_str': 'positive'},\n",
       " {'sequence': \"[CLS] I admit I had some trepidation when I first saw the previews for this film. Was VH - 1 treading on hollow ground here? I mean, Harris and Quinn don't really look or even sound like John or Paul. But I have to admit, this film really surprised me. It's far from the exploitation film I expected. Instead, it's a character study, a low - key, whimsical, and ultimately bittersweet look at friendship, and the ultimate lesson we all learn : it's hard, if not impossible, to capture what we once had, and what has passed us by. This is a negative review [SEP]\",\n",
       "  'score': 0.013285123743116856,\n",
       "  'token': 4366,\n",
       "  'token_str': 'negative'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "long_review = \"\"\"I admit I had some trepidation when I first saw the previews for this film. Was VH-1 treading on \n",
    "hollow ground here? I mean, Harris and Quinn don't really look or even sound like John or Paul. But I \n",
    "have to admit, this film really surprised me. It's far from the exploitation film I expected.\n",
    "Instead, it's a character study, a low-key, whimsical, and ultimately bittersweet look at \n",
    "friendship, and the ultimate lesson we all learn: it's hard, if not impossible, to capture what we \n",
    "once had, and what has passed us by.\"\"\"  # gold label: positive\n",
    "nlp(f\"{long_review} This is a {nlp.tokenizer.mask_token} review\", targets=[' positive', ' negative'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sequence': '[CLS] This film tried to be too many things all at once : stinging political satire, Hollywood blockbuster,... This is a negative review [SEP]',\n",
       "  'score': 0.056411635130643845,\n",
       "  'token': 4366,\n",
       "  'token_str': 'negative'},\n",
       " {'sequence': '[CLS] This film tried to be too many things all at once : stinging political satire, Hollywood blockbuster,... This is a positive review [SEP]',\n",
       "  'score': 0.03836614638566971,\n",
       "  'token': 3112,\n",
       "  'token_str': 'positive'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "long_review = \"\"\"This film tried to be too many things all at once: stinging political satire, Hollywood blockbuster, ...\"\"\"  # gold label: negative\n",
    "nlp(f\"{long_review} This is a {nlp.tokenizer.mask_token} review\", targets=[' positive', ' negative'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quora\n",
    "Similarity is ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sequence': '[CLS] Question 1 : Who is Bill Gates? Question 2 : Who is Elon Musk? Comment : Are these the same questions? No [SEP]',\n",
       "  'score': 1.9899765902664512e-05,\n",
       "  'token': 1302,\n",
       "  'token_str': 'No'},\n",
       " {'sequence': '[CLS] Question 1 : Who is Bill Gates? Question 2 : Who is Elon Musk? Comment : Are these the same questions? Yes [SEP]',\n",
       "  'score': 1.7568801922607236e-05,\n",
       "  'token': 2160,\n",
       "  'token_str': 'Yes'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp(f\"Question 1: Who is Bill Gates? Question 2: Who is Elon Musk? Comment: Are these the same questions? {nlp.tokenizer.mask_token}\", \n",
    "    targets=['Yes', 'No'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sequence': '[CLS] Question 1 : Who is Bill Gates? Question 2 : Who is the person named Bill Gates? Comment : Are these the same questions? No [SEP]',\n",
       "  'score': 2.5009821911226027e-05,\n",
       "  'token': 1302,\n",
       "  'token_str': 'No'},\n",
       " {'sequence': '[CLS] Question 1 : Who is Bill Gates? Question 2 : Who is the person named Bill Gates? Comment : Are these the same questions? Yes [SEP]',\n",
       "  'score': 1.9519991838023998e-05,\n",
       "  'token': 2160,\n",
       "  'token_str': 'Yes'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp(f\"Question 1: Who is Bill Gates? Question 2: Who is the person named Bill Gates? Comment: Are these the same questions? {nlp.tokenizer.mask_token}\", \n",
    "    targets=['Yes', 'No'])\n",
    "# The probabilities are very small. These are likely because the sentences do not follow natural grammar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sequence': '[CLS] Are \" Who is Bill Gates? \" and \" Who is the man named Bill Gates? \" the same questions? Yes [SEP]',\n",
       "  'score': 2.149405554519035e-05,\n",
       "  'token': 2160,\n",
       "  'token_str': 'Yes'},\n",
       " {'sequence': '[CLS] Are \" Who is Bill Gates? \" and \" Who is the man named Bill Gates? \" the same questions? No [SEP]',\n",
       "  'score': 1.904843702504877e-05,\n",
       "  'token': 1302,\n",
       "  'token_str': 'No'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp(f\"Are \\\"Who is Bill Gates?\\\" and \\\"Who is the man named Bill Gates?\\\" the same questions? {nlp.tokenizer.mask_token}\", \n",
    "    targets=['Yes', 'No'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sequence': '[CLS] Are \" Who is Bill Gates? \" and \" Who is Elon Musk? \" the same questions? No [SEP]',\n",
       "  'score': 1.9226656149839982e-05,\n",
       "  'token': 1302,\n",
       "  'token_str': 'No'},\n",
       " {'sequence': '[CLS] Are \" Who is Bill Gates? \" and \" Who is Elon Musk? \" the same questions? Yes [SEP]',\n",
       "  'score': 1.9152759705320932e-05,\n",
       "  'token': 2160,\n",
       "  'token_str': 'Yes'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp(f\"Are \\\"Who is Bill Gates?\\\" and \\\"Who is Elon Musk?\\\" the same questions? {nlp.tokenizer.mask_token}\", \n",
    "    targets=['Yes', 'No'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sequence': '[CLS] Are \" Who is Bill Gates? \" and \" Who is the founder of Microsoft? \" the same questions? Yes [SEP]',\n",
       "  'score': 1.9339111531735398e-05,\n",
       "  'token': 2160,\n",
       "  'token_str': 'Yes'},\n",
       " {'sequence': '[CLS] Are \" Who is Bill Gates? \" and \" Who is the founder of Microsoft? \" the same questions? No [SEP]',\n",
       "  'score': 1.872795655799564e-05,\n",
       "  'token': 1302,\n",
       "  'token_str': 'No'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp(f\"Are \\\"Who is Bill Gates?\\\" and \\\"Who is the founder of Microsoft?\\\" the same questions? {nlp.tokenizer.mask_token}\", \n",
    "    targets=['Yes', 'No'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLI\n",
    "NLI appears hard. Multiple types of anomalies occur when a semantic contradiction exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sequence': '[CLS] Does \" The scientists supported the doctors. \" entail \" The doctors supported the scientists. \"? No [SEP]',\n",
       "  'score': 1.100172994483728e-05,\n",
       "  'token': 1302,\n",
       "  'token_str': 'No'},\n",
       " {'sequence': '[CLS] Does \" The scientists supported the doctors. \" entail \" The doctors supported the scientists. \"? Yes [SEP]',\n",
       "  'score': 8.395280019612983e-06,\n",
       "  'token': 2160,\n",
       "  'token_str': 'Yes'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# HANS example from https://huggingface.co/datasets/viewer/?dataset=hans\n",
    "nlp(f\"Does \\\"The scientists supported the doctors.\\\" entail \\\"The doctors supported the scientists.\\\"? {nlp.tokenizer.mask_token}\", \n",
    "    targets=['Yes', 'No'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sequence': '[CLS] Does \" The scientists supported the doctors. \" entail \" The doctors were supported by the scientists. \"? No [SEP]',\n",
       "  'score': 7.5180846579314675e-06,\n",
       "  'token': 1302,\n",
       "  'token_str': 'No'},\n",
       " {'sequence': '[CLS] Does \" The scientists supported the doctors. \" entail \" The doctors were supported by the scientists. \"? Yes [SEP]',\n",
       "  'score': 5.064520337327849e-06,\n",
       "  'token': 2160,\n",
       "  'token_str': 'Yes'}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp(f\"Does \\\"The scientists supported the doctors.\\\" entail \\\"The doctors were supported by the scientists.\\\"? {nlp.tokenizer.mask_token}\", \n",
    "    targets=['Yes', 'No'])\n",
    "# These two sentences appear to have different syntax (but the same meaning)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gender Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sequence': '[CLS] A scientist came into the classroom. He started teaching. [SEP]',\n",
       "  'score': 0.5704528093338013,\n",
       "  'token': 1124,\n",
       "  'token_str': 'He'},\n",
       " {'sequence': '[CLS] A scientist came into the classroom. She started teaching. [SEP]',\n",
       "  'score': 0.10203046351671219,\n",
       "  'token': 1153,\n",
       "  'token_str': 'She'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp(f\"A scientist came into the classroom. {nlp.tokenizer.mask_token} started teaching.\", \n",
    "    targets=['He', 'She'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sequence': '[CLS] A scientist came into the classroom. He started crying. [SEP]',\n",
       "  'score': 0.2962949275970459,\n",
       "  'token': 1124,\n",
       "  'token_str': 'He'},\n",
       " {'sequence': '[CLS] A scientist came into the classroom. She started crying. [SEP]',\n",
       "  'score': 0.2684028148651123,\n",
       "  'token': 1153,\n",
       "  'token_str': 'She'}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp(f\"A scientist came into the classroom. {nlp.tokenizer.mask_token} started crying.\", \n",
    "    targets=['He', 'She'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sequence': '[CLS] Someone came into the room. He started teaching. [SEP]',\n",
       "  'score': 0.24824635684490204,\n",
       "  'token': 1124,\n",
       "  'token_str': 'He'},\n",
       " {'sequence': '[CLS] Someone came into the room. She started teaching. [SEP]',\n",
       "  'score': 0.10383621603250504,\n",
       "  'token': 1153,\n",
       "  'token_str': 'She'}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp(f\"Someone came into the room. {nlp.tokenizer.mask_token} started teaching.\", \n",
    "    targets=['He', 'She'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sequence': '[CLS] Someone came into the room. She started crying. [SEP]',\n",
       "  'score': 0.23532070219516754,\n",
       "  'token': 1153,\n",
       "  'token_str': 'She'},\n",
       " {'sequence': '[CLS] Someone came into the room. He started crying. [SEP]',\n",
       "  'score': 0.07483687251806259,\n",
       "  'token': 1124,\n",
       "  'token_str': 'He'}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp(f\"Someone came into the room. {nlp.tokenizer.mask_token} started crying.\", \n",
    "    targets=['He', 'She'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
