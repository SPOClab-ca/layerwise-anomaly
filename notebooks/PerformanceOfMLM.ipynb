{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance of MLM\n",
    "\n",
    "What is the performance of all of these tasks using MLM instead of GMM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, XLNetLMHeadModel\n",
    "\n",
    "import src.sentpair_generator\n",
    "import src.anomaly_model\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "pd.options.display.max_columns = 100\n",
    "pd.options.display.max_rows = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentgen = src.sentpair_generator.SentPairGenerator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter sentences that are in all of their vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_roberta = AutoTokenizer.from_pretrained('roberta-base')\n",
    "tok_bert = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "tok_xlnet = AutoTokenizer.from_pretrained('xlnet-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return true if the list of tokens differs in exactly one place\n",
    "def is_single_diff(toks1, toks2):\n",
    "  if len(toks1) != len(toks2):\n",
    "    return False\n",
    "  \n",
    "  diff_toks = 0\n",
    "  for ix in range(len(toks1)):\n",
    "    if toks1[ix] != toks2[ix]:\n",
    "      diff_toks += 1\n",
    "  \n",
    "  return diff_toks == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def works_for_model(tokenizer, sent1, sent2):\n",
    "  toks1 = tokenizer.tokenize(sent1)\n",
    "  toks2 = tokenizer.tokenize(sent2)\n",
    "  return is_single_diff(toks1, toks2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_pairs = defaultdict(list)\n",
    "for task_name, sent_pair_set in sentgen.get_hand_selected().items():\n",
    "  for sent1, sent2 in sent_pair_set.sent_pairs:\n",
    "    if works_for_model(tok_roberta, sent1, sent2) and \\\n",
    "        works_for_model(tok_bert, sent1, sent2) and\\\n",
    "        works_for_model(tok_xlnet, sent1, sent2):\n",
    "      sent_pairs[task_name].append((sent1, sent2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLiMP-SubjVerb 1542\n",
      "BLiMP-DetNoun 1813\n",
      "Osterhout-Nicol-Syntactic 71\n",
      "BLiMP-Animacy 658\n",
      "Pylkkanen 44\n",
      "Warren-Selectional 18\n",
      "Osterhout-Nicol-Semantic 69\n",
      "Osterhout-Mobley 53\n",
      "Warren-Pragmatic 20\n",
      "CPRAG-34 24\n",
      "Urbach 66\n"
     ]
    }
   ],
   "source": [
    "for task_name, sent_pair_set in sent_pairs.items():\n",
    "  print(task_name, len(sent_pair_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill Mask Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "nlp = pipeline(\"fill-mask\", model='bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_one(sent1, sent2):\n",
    "  toks1 = nlp.tokenizer(sent1, add_special_tokens=False)['input_ids']\n",
    "  toks2 = nlp.tokenizer(sent2, add_special_tokens=False)['input_ids']\n",
    "\n",
    "  masked_toks = []\n",
    "  dtok1 = None\n",
    "  dtok2 = None\n",
    "  for ix in range(len(toks1)):\n",
    "    if toks1[ix] != toks2[ix]:\n",
    "      masked_toks.append(nlp.tokenizer.mask_token_id)\n",
    "      dtok1 = toks1[ix]\n",
    "      dtok2 = toks2[ix]\n",
    "    else:\n",
    "      masked_toks.append(toks1[ix])\n",
    "\n",
    "  res = nlp(nlp.tokenizer.decode(masked_toks), targets=[nlp.tokenizer.decode(dtok1), nlp.tokenizer.decode(dtok2)])\n",
    "  return res[0]['token'] == dtok1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlm_accuracy(sentpairs):\n",
    "  res = [fill_one(s1, s2) for (s1, s2) in sentpairs]\n",
    "  return sum(res) / len(sentpairs)\n",
    "\n",
    "for task_name, sents in sent_pairs.items():\n",
    "  print(task_name, mlm_accuracy(sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XLNet needs to be done differently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'xlnet-base-cased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = XLNetLMHeadModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_one(sent1, sent2):\n",
    "  toks1 = tokenizer(sent1, add_special_tokens=False)['input_ids']\n",
    "  toks2 = tokenizer(sent2, add_special_tokens=False)['input_ids']\n",
    "\n",
    "  masked_toks = []\n",
    "  masked_ix = None\n",
    "  dtok1 = None\n",
    "  dtok2 = None\n",
    "  for ix in range(len(toks1)):\n",
    "    if toks1[ix] != toks2[ix]:\n",
    "      masked_toks.append(tokenizer.mask_token_id)\n",
    "      masked_ix = ix\n",
    "      dtok1 = toks1[ix]\n",
    "      dtok2 = toks2[ix]\n",
    "    else:\n",
    "      masked_toks.append(toks1[ix])\n",
    "\n",
    "  logit1 = model(torch.tensor([masked_toks])).logits[0, masked_ix, dtok1]\n",
    "  logit2 = model(torch.tensor([masked_toks])).logits[0, masked_ix, dtok2]\n",
    "  return bool(logit1 > logit2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlm_accuracy(sentpairs):\n",
    "  res = [fill_one(s1, s2) for (s1, s2) in sentpairs]\n",
    "  return sum(res) / len(sentpairs)\n",
    "\n",
    "for task_name, sents in sent_pairs.items():\n",
    "  print(task_name, mlm_accuracy(sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try using Gaussian model on same data, use best layer for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/bnc.pkl', 'rb') as f:\n",
    "  bnc_sentences = pickle.load(f)\n",
    "\n",
    "random.seed(12345)\n",
    "bnc_sentences = random.sample(bnc_sentences, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'bert-base-uncased'\n",
    "MODEL_LAYER = 9\n",
    "\n",
    "model = src.anomaly_model.AnomalyModel(bnc_sentences, model_name=MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sentpair_dataset(taskname, sent_pairs):\n",
    "  scores = []\n",
    "  for layer in [MODEL_LAYER]:\n",
    "    results = model.eval_sent_pairs(sent_pairs, layer)\n",
    "    scores.extend([{'taskname': taskname, 'layer': layer, 'score': r} for r in results])\n",
    "  scores = pd.DataFrame(scores)\n",
    "  return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores = []\n",
    "for taskname, sentpairs in sent_pairs.items():\n",
    "  task_scores = process_sentpair_dataset(taskname, sentpairs)\n",
    "  all_scores.append(task_scores)\n",
    "  \n",
    "# Role-88 is special...\n",
    "#taskname = 'ROLE-88'\n",
    "#sentpairs = sentgen.get_hand_selected()['ROLE-88']\n",
    "#task_scores = process_sentpair_dataset(taskname, sentpairs.sent_pairs)\n",
    "#all_scores.append(task_scores)\n",
    "  \n",
    "all_scores = pd.concat(all_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores['Correct'] = all_scores.score > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Correct</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>taskname</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ROLE-88</th>\n",
       "      <td>0.590909</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Correct\n",
       "taskname          \n",
       "ROLE-88   0.590909"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_scores[['taskname', 'Correct']].groupby('taskname', sort=False).mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
