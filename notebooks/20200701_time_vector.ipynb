{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Vector\n",
    "In this notebook, focus only on the time vector. The hypothesis is: The vector difference bert(\"I eat lunch\") - bert(\"I ate lunch\") resides on some concentrated directions, and is disentangled from other factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import pickle\n",
    "import transformers\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils import timed_func\n",
    "\n",
    "tags_to_tense = {\n",
    "    \"VB\": \"base\", \n",
    "    \"VBD\": \"past\",\n",
    "    \"VBG\": \"present_participle\",\n",
    "    \"VBN\": \"past_participle\",\n",
    "    \"VBP\": \"non_3rd_singular_present\",\n",
    "    \"VBZ\": \"3rd_singular_present\"\n",
    "}\n",
    "verb_tags = [\"VB\", \"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d65b455714a4613981a9e853f2b9c27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=213450.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "308849e68ebb41ad90005e9bea3e8d05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad97215d2a604f4097cdeb1fc9238621",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=435779157.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-cased', output_hidden_states=False)\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SST contains 11978 sentences, avg words 18.97, stdvar 9.24.\n",
      "read_sst_sentences_plaintext done in 0.44 seconds.\n"
     ]
    }
   ],
   "source": [
    "@timed_func\n",
    "def read_sst_sentences(verbose=True):\n",
    "    \"\"\"\n",
    "    Output: list of spacy-processed Docs\n",
    "    \"\"\"\n",
    "    L = []\n",
    "    wordcounts = []\n",
    "    with open(\"../data/stanfordSentimentTreebank/datasetSentences.txt\", \"r\") as f:\n",
    "        for line in f.readlines():\n",
    "            raw_text = line.split(\"\\t\")[1]\n",
    "            for sent in sent_tokenize(raw_text):\n",
    "                doc = nlp(sent)\n",
    "                L.append(doc)\n",
    "                wordcounts.append(len(doc))\n",
    "    if verbose:\n",
    "        print (\"SST contains {} sentences, avg words {:.2f}, stdvar {:.2f}.\".format(\n",
    "            len(wordcounts), np.mean(wordcounts), np.std(wordcounts)\n",
    "        ))\n",
    "    return L\n",
    "\n",
    "@timed_func\n",
    "def read_sst_sentences_plaintext(verbose=True):\n",
    "    \"\"\"\n",
    "    Output: list of string\n",
    "    \"\"\"\n",
    "    L = []\n",
    "    wordcounts = []\n",
    "    with open(\"../data/stanfordSentimentTreebank/datasetSentences.txt\", \"r\") as f:\n",
    "        for line in f.readlines():\n",
    "            raw_text = line.split(\"\\t\")[1]\n",
    "            for sent in sent_tokenize(raw_text):\n",
    "                L.append(sent)\n",
    "                wordcounts.append(len(sent.split()))\n",
    "    if verbose:\n",
    "        print (\"SST contains {} sentences, avg words {:.2f}, stdvar {:.2f}.\".format(\n",
    "            len(wordcounts), np.mean(wordcounts), np.std(wordcounts)\n",
    "        ))\n",
    "    return L\n",
    "\n",
    "#sst_sentences = read_sst_sentences()\n",
    "sst_plaintext = read_sst_sentences_plaintext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sst_sentences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-2bd716a306e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtags_freq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtags_freq\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount_tags_freq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msst_sentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sst_sentences' is not defined"
     ]
    }
   ],
   "source": [
    "@timed_func\n",
    "def count_tags_freq(sentences, verb_only=True):\n",
    "    tags_freq = {}\n",
    "    for sent in sentences:\n",
    "        for token in sent:\n",
    "            t = token.tag_\n",
    "            if t not in tags_freq:\n",
    "                tags_freq[t] = 1\n",
    "            else:\n",
    "                tags_freq[t] += 1\n",
    "                \n",
    "    if verb_only:\n",
    "        return [(k,tags_freq[k]) for k in tags]\n",
    "    else:\n",
    "        return [(k,tags_freq[k]) for k in tags_freq]\n",
    "\n",
    "print(count_tags_freq(sst_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timed_func\n",
    "def histogram_sentence_tenses(sentences):\n",
    "    # Count how many tenses each sentence has.\n",
    "    num_tenses_histogram = {}\n",
    "    for sent in sentences:\n",
    "        indicator = [0] * len(verb_tags)\n",
    "        for token in sent:\n",
    "            if token.tag_ in verb_tags:\n",
    "                indicator[tags.index(token.tag_)] = 1\n",
    "        num = np.array(indicator).sum()\n",
    "        if num not in num_tenses_histogram:\n",
    "            num_tenses_histogram[num] = 1\n",
    "            print(f\"Example of {num} tenses:\", sent, [token.tag_ for token in sent])\n",
    "        else:\n",
    "            num_tenses_histogram[num] += 1\n",
    "    print(num_tenses_histogram)\n",
    "    \n",
    "histogram_sentence_tenses(sst_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some observations\n",
    "1. There are many annotation errors in determining verb tenses. Need manual checks. (but the dataset size is only several thousands. Can be done within hours)  \n",
    "2. Let me systematically determine the tense of a sentence with the tense of **the outmost verb** in constituency parsing.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timed_func\n",
    "def filter_past_tense_sentences_v1(sentences):\n",
    "    # Intuition 1: if any \"past tense\" (VBD) verb form exists. This does not work well.\n",
    "    results = []\n",
    "    for sent in sentences:\n",
    "        tags = [token.tag_ for token in sent]\n",
    "        \n",
    "        if \"VBD\" in tags:\n",
    "            results.append(str(sent)+\"\\n\")\n",
    "        \n",
    "        \n",
    "    with open(os.path.join(\"20200701_outputs\", \"past_tense.txt\"), \"w+\") as f:\n",
    "        f.writelines(results)\n",
    "\n",
    "filter_past_tense_sentences(sst_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00a7829758e74dfcb542a71841b0f71e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=710808161.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "671b433d9a9240348748070bd75b1032",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=336.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acde2e0e873b4bd2bdcbf7d01414de4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=374434792.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from allennlp.predictors.predictor import Predictor\n",
    "import allennlp_models.structured_prediction\n",
    "allennlp_predictor = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/elmo-constituency-parser-2020.02.10.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your label namespace was 'pos'. We recommend you use a namespace ending with 'labels' or 'tags', so we don't add UNK and PAD tokens by default to your vocabulary.  See documentation for `non_padded_namespaces` parameter in Vocabulary.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('VB', 'buy', 'base')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_main_verb_and_tense(sentence):\n",
    "    res = allennlp_predictor.predict(sentence=sentence)\n",
    "    tr = nltk.tree.Tree.fromstring(res['trees'])\n",
    "    treepositions = sorted(tr.treepositions(), key=lambda tup: len(tup))  # Level-order traversal. \n",
    "    for pos in treepositions:\n",
    "        node = tr[pos]\n",
    "        depth = len(pos)\n",
    "        if hasattr(node, \"label\") and node.label() in verb_tags:  # The first occurrence\n",
    "            return node.label(), node[0], tags_to_tense[node.label()]\n",
    "    return None, None, None\n",
    "                \n",
    "find_main_verb_and_tense(\"If I brought you 10 dollars, can you buy me lunch?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('VBZ', 'is', '3rd_singular_present')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_main_verb_and_tense(\"The gorgeously elaborate continuation of `` The Lord of the Rings '' trilogy is so huge that a column of words can not adequately describe co-writer\\/director Peter Jackson 's expanded vision of J.R.R.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11978/11978 [1:59:14<00:00,  1.67it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute_tenses done in 7155.02 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "@timed_func\n",
    "def compute_tenses(plaintexts):\n",
    "    tense_results = {}\n",
    "    for text in tqdm(plaintexts):\n",
    "        # Use the tense of the main verb \n",
    "        # (where the main verb is the highest verb in constituency parsing tree)\n",
    "        verbtag, verb, tense = find_main_verb_and_tense(text)\n",
    "        if verbtag is not None:\n",
    "            if verbtag not in tense_results:\n",
    "                tense_results[verbtag] = [(text, verb)]\n",
    "            else:\n",
    "                tense_results[verbtag].append(tuple([text, verb]))\n",
    "    with open(\"20200701_outputs/tense_results.pkl\", \"wb+\") as f:\n",
    "        pickle.dump(tense_results, f)\n",
    "        \n",
    "compute_tenses(sst_plaintext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "VBZ: 3rd_singular_present, 6565 sentences\n",
      "(\"The Rock is destined to be the 21st Century 's new `` Conan '' and that he 's going to make a splash even greater than Arnold Schwarzenegger , Jean-Claud Van Damme or Steven Segal .\", 'is')\n",
      "(\"The gorgeously elaborate continuation of `` The Lord of the Rings '' trilogy is so huge that a column of words can not adequately describe co-writer\\\\/director Peter Jackson 's expanded vision of J.R.R.\", 'is')\n",
      "('If you sometimes like to go to the movies to have fun , Wasabi is a good place to start .', 'is')\n",
      "('The film provides some great insight into the neurotic mindset of all comics -- even those who have reached the absolute top of the game .', 'provides')\n",
      "\n",
      "VB: base, 1372 sentences\n",
      "(\"Emerges as something rare , an issue movie that 's so honest and keenly observed that it does n't feel like one .\", 'Emerges')\n",
      "(\"The movie 's ripe , enrapturing beauty will tempt those willing to probe its inscrutable mysteries .\", 'tempt')\n",
      "('Fuller would surely have called this gutsy and at times exhilarating movie a great yarn .', 'have')\n",
      "(\"May be spoofing an easy target -- those old ' 50 's giant creature features -- but ... it acknowledges and celebrates their cheesiness as the reason why people get a kick out of watching them today .\", 'be')\n",
      "\n",
      "VBN: past_participle, 524 sentences\n",
      "('One of the greatest family-oriented , fantasy-adventure movies ever .', 'oriented')\n",
      "('A disturbing and frighteningly evocative assembly of imagery and hypnotic music composed by Philip Glass .', 'composed')\n",
      "('Guaranteed to move anyone who ever shook , rattled , or rolled .', 'Guaranteed')\n",
      "('A pleasant enough movie , held together by skilled ensemble actors .', 'held')\n",
      "\n",
      "VBD: past, 474 sentences\n",
      "(\"An utterly compelling ` who wrote it ' in which the reputation of the most famous author who ever lived comes into question .\", 'wrote')\n",
      "('Though everything might be literate and smart , it never took off and always seemed static .', 'took')\n",
      "('I loved it !', 'loved')\n",
      "('I enjoyed Time of Favor while I was watching it , but I was surprised at how quickly it faded from my memory .', 'enjoyed')\n",
      "\n",
      "VBG: present_participle, 566 sentences\n",
      "('Illuminating if overly talky documentary .', 'Illuminating')\n",
      "('A thoughtful , provocative , insistently humanizing film .', 'humanizing')\n",
      "('Scores a few points for doing what it does with a dedicated and good-hearted professionalism .', 'doing')\n",
      "('A simmering psychological drama in which the bursts of sudden violence are all the more startling for the slow buildup that has preceded them .', 'simmering')\n",
      "\n",
      "VBP: non_3rd_singular_present, 1138 sentences\n",
      "(\"If there 's a way to effectively teach kids about the dangers of drugs , I think it 's in projects like the -LRB- unfortunately R-rated -RRB- Paid .\", 'think')\n",
      "('Like most Bond outings in recent years , some of the stunts are so outlandish that they border on being cartoonlike .', 'are')\n",
      "(\"Some actors have so much charisma that you 'd be happy to listen to them reading the phone book .\", 'have')\n",
      "('Hugh Grant and Sandra Bullock are two such likeable actors .', 'are')\n"
     ]
    }
   ],
   "source": [
    "with open(\"20200701_outputs/tense_results.pkl\", \"rb\") as f:\n",
    "    tense_results = pickle.load(f)\n",
    "for verbtag in tense_results:\n",
    "    print(\"\\n{}: {}, {} sentences\".format(verbtag, tags_to_tense[verbtag], len(tense_results[verbtag])))\n",
    "    print(tense_results[verbtag][0])\n",
    "    print(tense_results[verbtag][1])\n",
    "    print(tense_results[verbtag][2])\n",
    "    print(tense_results[verbtag][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
