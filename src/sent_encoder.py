from transformers import AutoTokenizer, AutoModel
import numpy as np
import torch

BATCH_SIZE = 32

class SentEncoder:
  def __init__(self, model_name='roberta-base'):
    self.model_name = model_name
    self.auto_tokenizer = AutoTokenizer.from_pretrained(model_name)
    self.auto_model = AutoModel.from_pretrained(model_name).cuda()

  def evaluate_contextual_diff(self, pairs, layer=-1):
    """Get sentence embedding difference between pairs of sentences, for a given layer.
    Sentence embeddings are generated by averaging across contextual word embeddings."""
    result = []
    for batch_ix in range(0, len(pairs), BATCH_SIZE):
      batch_sentences = pairs[batch_ix : batch_ix+BATCH_SIZE]
      src_sentences = [p[0] for p in batch_sentences]
      tgt_sentences = [p[1] for p in batch_sentences]

      src_ids = torch.tensor(self.auto_tokenizer(src_sentences, padding=True)['input_ids']).cuda()
      tgt_ids = torch.tensor(self.auto_tokenizer(tgt_sentences, padding=True)['input_ids']).cuda()

      src_vecs = self.auto_model(src_ids, output_hidden_states=True)[2][layer].mean(dim=1)
      tgt_vecs = self.auto_model(tgt_ids, output_hidden_states=True)[2][layer].mean(dim=1)
      diff_vecs = src_vecs - tgt_vecs
      result.append(diff_vecs.cpu().detach().numpy())

    return np.vstack(result)
