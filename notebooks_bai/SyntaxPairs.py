#!/usr/bin/env python
# coding: utf-8

# # Distribution of cosine distance for minimal syntax pairs

# In[11]:


from transformers import AutoTokenizer, AutoModel
import torch
import numpy as np
import pandas as pd
import os, sys, time, re
import matplotlib.pyplot as plt
import random
import pickle
from scipy.spatial.distance import cosine
import seaborn as sns

get_ipython().run_line_magic('matplotlib', 'inline')


# ## Load sentence pairs
# 
# "Targeted Syntactic Evaluation of Language Models" by Marvin and Linzen (2018).
# https://github.com/BeckyMarvin/LM_syneval
# 
# Or use the ones generated by CheckList.

# In[2]:


with open('../data/sents.pkl', 'rb') as f:
  data = pickle.load(f)
  data = list(data)


# In[8]:


# https://wortschatz.uni-leipzig.de/en/download/english
# Wikipedia, 2016, 10K sentences
with open('../data/leipzig_wikipedia.txt') as f:
  wiki_sents = f.read().split('\n')[:-1]


# In[9]:


len(data)


# In[8]:


data[:5]


# In[10]:


wiki_sents[:5]


# ## Load RoBERTa

# In[17]:


model_name = 'roberta-base'
bert_tokenizer = AutoTokenizer.from_pretrained(model_name)
bert_model = AutoModel.from_pretrained(model_name)


# In[15]:


def evaluate_contextual_diff(pair):
    source, target = pair[0], pair[1]
    src_ids = torch.tensor(bert_tokenizer.encode(source)).unsqueeze(0)
    src_vec = bert_model(src_ids)[0].mean(dim=1)[0]  # (768,) torch.tensor
    
    tgt_ids = torch.tensor(bert_tokenizer.encode(target)).unsqueeze(0)
    tgt_vec = bert_model(tgt_ids)[0].mean(dim=1)[0]
    
    d_emb = len(src_vec)  # 768
    diff = src_vec - tgt_vec
    return diff # 768-dimensional torch.tensor


# ## Wikipedia as control sentences

# In[29]:


wiki_vecs = []
for i in range(300):
  s1 = random.choice(wiki_sents)
  s2 = random.choice(wiki_sents)
  if s1 == s2: continue
  wiki_vecs.append(evaluate_contextual_diff((s1, s2)).detach().numpy())
wiki_vecs = np.stack(wiki_vecs)


# In[30]:


wiki_cosine_distances = []
for i in range(len(wiki_vecs)):
  for j in range(i+1, len(wiki_vecs)):
    wiki_cosine_distances.append(cosine(wiki_vecs[i,:], wiki_vecs[j,:]))


# ## Evaluate and plot

# In[21]:


vecs = np.stack([evaluate_contextual_diff(pair).detach().numpy() for pair in data])


# In[22]:


cosine_distances = []
for i in range(len(vecs)):
  for j in range(i+1, len(vecs)):
    cosine_distances.append(pd.Series({
      'cosdist': cosine(vecs[i,:], vecs[j,:]),
      'sent1a': data[i][0],
      'sent1b': data[i][1],
      'sent2a': data[j][0],
      'sent2b': data[j][1],
    }))


# In[23]:


cosine_distances = pd.DataFrame(cosine_distances)


# In[36]:


sns.distplot(cosine_distances.cosdist, bins=40, label='dobj/iobj pairs')
sns.distplot(wiki_cosine_distances, bins=40, label='random wikipedia sentences')
plt.title(f"Using model: {model_name}")
plt.legend()
plt.show()


# In[25]:


np.mean(cosine_distances.cosdist)


# In[33]:


np.mean(wiki_cosine_distances)


# ## Inspect most similar and dissimilar pairs
# 
# Some sentences seem questionable: "The man took the dog a walk".

# In[55]:


cosine_distances.sort_values('cosdist').head(5)


# In[58]:


cosine_distances.sort_values('cosdist', ascending=False).head(5)

