#!/usr/bin/env python
# coding: utf-8

# # Distribution of cosine distance for minimal syntax pairs

# In[1]:


import sys
sys.path.append('../')

from transformers import AutoTokenizer, AutoModel
import torch
import numpy as np
import pandas as pd
import os, sys, time, re
import matplotlib.pyplot as plt
import random
import pickle
from scipy.spatial.distance import cosine
import seaborn as sns

import src.sent_encoder

get_ipython().run_line_magic('matplotlib', 'inline')
get_ipython().run_line_magic('load_ext', 'autoreload')
get_ipython().run_line_magic('autoreload', '2')


# ## Load sentence pairs
# 
# "Targeted Syntactic Evaluation of Language Models" by Marvin and Linzen (2018).
# https://github.com/BeckyMarvin/LM_syneval
# 
# Or use the ones generated by CheckList.

# In[2]:


with open('../data/sents.pkl', 'rb') as f:
  data = pickle.load(f)
  data = list(data)


# In[3]:


# https://wortschatz.uni-leipzig.de/en/download/english
# Wikipedia, 2016, 10K sentences
with open('../data/leipzig_wikipedia.txt') as f:
  wiki_sents = f.read().split('\n')[:-1]


# In[4]:


len(data)


# In[5]:


data[:5]


# In[6]:


wiki_sents[:5]


# In[7]:


enc = src.sent_encoder.SentEncoder()


# ## Wikipedia as control sentences

# In[8]:


wiki_vecs = []
for i in range(300):
  s1 = random.choice(wiki_sents)
  s2 = random.choice(wiki_sents)
  if s1 == s2: continue
  wiki_vecs.append(enc.evaluate_contextual_diff((s1, s2)).detach().numpy())
wiki_vecs = np.stack(wiki_vecs)


# In[9]:


wiki_cosine_distances = []
for i in range(len(wiki_vecs)):
  for j in range(i+1, len(wiki_vecs)):
    wiki_cosine_distances.append(cosine(wiki_vecs[i,:], wiki_vecs[j,:]))


# ## Evaluate and plot

# In[10]:


vecs = np.stack([enc.evaluate_contextual_diff(pair).detach().numpy() for pair in data])


# In[11]:


cosine_distances = []
for i in range(len(vecs)):
  for j in range(i+1, len(vecs)):
    cosine_distances.append(pd.Series({
      'cosdist': cosine(vecs[i,:], vecs[j,:]),
      'sent1a': data[i][0],
      'sent1b': data[i][1],
      'sent2a': data[j][0],
      'sent2b': data[j][1],
    }))


# In[12]:


cosine_distances = pd.DataFrame(cosine_distances)


# In[14]:


model_name = 'roberta-base'
sns.distplot(cosine_distances.cosdist, bins=40, label='dobj/iobj pairs')
sns.distplot(wiki_cosine_distances, bins=40, label='random wikipedia sentences')
plt.title(f"Using model: {model_name}")
plt.legend()
plt.show()


# In[15]:


np.mean(cosine_distances.cosdist)


# In[16]:


np.mean(wiki_cosine_distances)


# ## Inspect most similar and dissimilar pairs
# 
# Some sentences seem questionable: "The man took the dog a walk".

# In[17]:


cosine_distances.sort_values('cosdist').head(5)


# In[18]:


cosine_distances.sort_values('cosdist', ascending=False).head(5)

